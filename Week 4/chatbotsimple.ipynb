{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Chatbot\n",
    "In this video We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n",
    "\n",
    "Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n",
    "\n",
    "- Conversational RAG: Enable a chatbot experience over an external source of data\n",
    "- Agents: Build a chatbot that can take actions\n",
    "\n",
    "This video tutorial will cover the basics which will be helpful for those two more advanced topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() ## aloading all the environment variable\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000021633A5AE30>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002164D992560>, model_name='openai/gpt-oss-20b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"openai/gpt-oss-20b\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain uses structured message objects (HumanMessage, AIMessage, SystemMessage) instead of raw text.\n",
    "\n",
    "This enables multi-turn context directly.\n",
    "\n",
    "LangChain defines different message classes to represent different roles or sources of a message.\n",
    "\n",
    "These are:\n",
    "* HumanMessage üë§ ‚Äî what the user says\n",
    "* AIMessage ü§ñ ‚Äî what the AI responds\n",
    "* SystemMessage ‚öôÔ∏è ‚Äî instructions or context for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Jhon! üëã It‚Äôs great to meet an AI Engineer. How can I assist you today? Whether you‚Äôre working on a project, need insights on the latest research, or just want to brainstorm ideas, I‚Äôm here to help.', additional_kwargs={'reasoning_content': 'User says \"Hi , My name is Jhon and I am an AI Engineer\". They introduced themselves. We should respond. The user is Jhon. We can ask what they\\'d like to discuss or help with. Also note that the user might want a conversation. We\\'ll greet them and ask how we can help.'}, response_metadata={'token_usage': {'completion_tokens': 123, 'prompt_tokens': 84, 'total_tokens': 207, 'completion_time': 0.123429981, 'prompt_time': 0.003983243, 'queue_time': 0.00310321, 'total_time': 0.127413224}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_80501ff3a1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--b85ddd70-8e7e-4fe2-b595-47ae5c75c3c4-0', usage_metadata={'input_tokens': 84, 'output_tokens': 123, 'total_tokens': 207})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "model.invoke([HumanMessage(content=\"Hi , My name is Jhon and I am an AI Engineer\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I‚Äôm not sure what your name is or what you do‚Äîcould you let me know a bit more? Once I have that, I‚Äôll be happy to help or chat about it!', additional_kwargs={'reasoning_content': 'The user says: \"Hey What\\'s my name and what do I do?\" There\\'s no context. The user presumably wants to know their name and what they do. But we don\\'t have that info. We should ask for clarification. We can respond: I don\\'t know your name or what you do, but I\\'d be happy to help if you tell me. Probably the best answer: I\\'m not aware of your name or what you do; please provide more context.'}, response_metadata={'token_usage': {'completion_tokens': 139, 'prompt_tokens': 81, 'total_tokens': 220, 'completion_time': 0.137904112, 'prompt_time': 0.005100446, 'queue_time': 0.033860283, 'total_time': 0.143004558}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--73b67eb0-aaae-4c40-9ee9-f480833c6c33-0', usage_metadata={'input_tokens': 81, 'output_tokens': 139, 'total_tokens': 220})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi , My name is Jhon and I am an AI Engineer\"),\n",
    "        AIMessage(content=\"Hello Jhon! It's nice to meet you. \\n\\nAs an AI Engineer, what kind of projects are you working on these days? \\n\\nI'm always eager to learn more about the exciting work being done in the field of AI.\\n\"),\n",
    "        HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message History\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You create a store (dictionary) that maps session IDs to chat histories.\n",
    "\n",
    "* RunnableWithMessageHistory automatically loads/saves conversation messages between turns.\n",
    "\n",
    "* Now, your model ‚Äúremembers‚Äù previous messages per session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_community) (1.0.1)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_community) (2.0.44)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_community) (3.13.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_community) (2.11.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_community) (0.4.38)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_community) (2.2.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.12.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (25.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\n",
      "Requirement already satisfied: anyio in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (4.11.0)\n",
      "Requirement already satisfied: certifi in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.3.0)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.3.1)\n",
      "Using cached langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "Installing collected packages: langchain_community\n",
      "Successfully installed langchain_community-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in-memory message store.\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "#wraps any chain or model\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "#session store\n",
    "store={}\n",
    "\n",
    "\n",
    "#Checks whether the session (like \"chat1\") already has a history.\n",
    "#If not, it creates a new empty one.\n",
    "#Returns the history object for that session.\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "#wraps your LLM model (model) with memory behavior\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time you call invoke() on with_message_history, LangChain automatically:\n",
    "* Looks up the session using get_session_history\n",
    "* Loads all previous messages for that session\n",
    "* Adds the new user input (HumanMessage)\n",
    "* Sends everything to the model\n",
    "* Saves the new model response (AIMessage) back into the same history\n",
    "\n",
    "Now our model ‚Äúremembers‚Äù earlier turns automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which session to use when calling invoke()\n",
    "\n",
    "config={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "\n",
    "#The key \"configurable\" is required by LangChain‚Äôs configuration system.\n",
    "#The \"session_id\" identifies which conversation history to use.\n",
    "#If you pass \"chat2\" instead, it will create a new independent chat memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , My name is Jhon and I am an AI Engineer\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH MESSAGEhistory(config(sessionid))->model+getsession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens here:\n",
    "* LangChain sees session_id = \"chat1\".\n",
    "* It calls get_session_history(\"chat1\") ‚Üí new history is created.\n",
    "* The input message (HumanMessage) is added to the history.\n",
    "* The model generates a reply (AIMessage).\n",
    "* The AIMessage is stored back in the same history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, Jhon! üëã It‚Äôs great to meet another AI enthusiast. How can I assist you today? Whether you‚Äôre working on a new project, troubleshooting code, or just curious about the latest in AI, I‚Äôm here to help!'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You mentioned your name is **Jhon**.', additional_kwargs={'reasoning_content': 'User says \"My name is Jhon\". So answer is Jhon.'}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 148, 'total_tokens': 183, 'completion_time': 0.0351388, 'prompt_time': 0.009955989, 'queue_time': 0.002859986, 'total_time': 0.045094789}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--0d4e02e2-e87d-401f-90bb-340f1ed9485f-0', usage_metadata={'input_tokens': 148, 'output_tokens': 35, 'total_tokens': 183})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens now:\n",
    "* LangChain again uses the same session_id = \"chat1\".\n",
    "* It loads previous messages from history (the ‚ÄúHi, my name is ‚Ä¶‚Äù exchange).\n",
    "* It appends the new HumanMessage(\"What's my name?\").\n",
    "* Sends the full conversation to the model.\n",
    "* The model can recall  name from the earlier part of the chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I don‚Äôt actually have that information. Could you let me know what you‚Äôd like me to call you?'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the config-->session id\n",
    "config11={\"configurable\":{\"session_id\":\"chat22\"}} #config->{\"configurable\":{\"session_id\":\"________\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config11\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Jack! How can I help you today?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My name is Jack\")],\n",
    "    config=config11\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is John.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "| Component                                        | What it does                        | Key Role                        |\n",
    "| ------------------------------------------------ | ----------------------------------- | ------------------------------- |\n",
    "| `ChatMessageHistory`                             | Stores messages in memory           | Acts like chat memory           |\n",
    "| `BaseChatMessageHistory`                         | Interface type                      | Ensures consistent structure    |\n",
    "| `store = {}`                                     | In-memory session map               | Holds all chat sessions         |\n",
    "| `get_session_history()`                          | Retrieves or creates session memory | Session-level memory management |\n",
    "| `RunnableWithMessageHistory`                     | Wraps model to add memory logic     | Automatically remembers chats   |\n",
    "| `config = {\"configurable\": {\"session_id\": ...}}` | Identifies chat session             | Links calls to the same memory  |\n",
    "| `invoke()`                                       | Runs the model with message history | Main entry point                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You define a system message for global behavior.\n",
    "\n",
    "MessagesPlaceholder acts as a slot where conversation history (human + AI messages) will be inserted dynamically.\n",
    "\n",
    "The | operator chains components (prompt ‚Üí model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Amnswer all the question to the nest of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Jhon! üëã How can I help you today?', additional_kwargs={'reasoning_content': 'User says \"Hi My name is Jhon\". Likely greeting. Need to respond. Possibly ask how to help.'}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 98, 'total_tokens': 145, 'completion_time': 0.046152953, 'prompt_time': 0.004692686, 'queue_time': 0.003334207, 'total_time': 0.050845639}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--35b24273-4aad-4f74-afa7-e79592c70398-0', usage_metadata={'input_tokens': 98, 'output_tokens': 47, 'total_tokens': 145})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Jhon\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine this with your RunnableWithMessageHistory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Jhon! üëã How can I help you today?', additional_kwargs={'reasoning_content': 'We need to respond. The user says \"Hi My name is Jhon\". Likely a greeting. We can reply. The instruction says \"You are a helpful assistant. Answer all the question to the best of your ability.\" There\\'s no question. Just greet. We can respond politely. Maybe ask how can help.'}, response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 98, 'total_tokens': 185, 'completion_time': 0.086407487, 'prompt_time': 0.005399963, 'queue_time': 0.002957473, 'total_time': 0.09180745}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_80501ff3a1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--91069f6c-eb1a-40f1-8d21-33c496399a49-0', usage_metadata={'input_tokens': 98, 'output_tokens': 87, 'total_tokens': 185})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is Jhon\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Jhon.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add more complexity\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡¥π‡¥æ‡¥Ø‡µç ‡¥ú‡µã‡µ∫! üëã ‡¥é‡¥®‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÜ ‡¥é‡¥ô‡µç‡¥ô‡¥®‡µÜ ‡¥∏‡¥π‡¥æ‡¥Ø‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥Ç?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Jhon\")],\"language\":\"Malayalam\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now wrap this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡¥π‡¥æ‡¥Ø‡µç ‡¥ú‡µá‡¥ï‡µç‡¥ï‡µç! ‡¥é‡¥ô‡µç‡¥ô‡¥®‡µÜ‡¥Ø‡µÅ‡¥£‡µç‡¥ü‡µç? ‡¥á‡¥®‡µç‡¥®‡µç ‡¥é‡¥®‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÜ ‡¥é‡¥ô‡µç‡¥ô‡¥®‡µÜ ‡¥∏‡¥π‡¥æ‡¥Ø‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥Ç?'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "repsonse=with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Hi,I am Jake\")],\"language\":\"Malayalam\"},\n",
    "    config=config\n",
    ")\n",
    "repsonse.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"Arabic\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ÿπÿ∞ÿ±ÿßŸãÿå ŸÑÿß ÿ£ÿπÿ±ŸÅ ÿßÿ≥ŸÖŸÉ. ÿ•ÿ∞ÿß ÿ±ÿ∫ÿ®ÿ™ ŸÅŸä ÿ•ÿÆÿ®ÿßÿ±Ÿäÿå ÿ≥ÿ£ŸÉŸàŸÜ ÿ≥ÿπŸäÿØÿßŸã ÿ®ŸÖÿπÿ±ŸÅÿ© ÿßÿ≥ŸÖŸÉ!'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing the Conversation History\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "'trim_messages' helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=40,\n",
    "    strategy=\"last\", #first last middle\n",
    "    token_counter=model, # how tokens are counted\n",
    "    include_system=True, #whether the system messageshould always be kept.\n",
    "    allow_partial=False,#whether a single long message can be partially truncated\n",
    "    start_on=\"human\" #where to start counting tokens from\n",
    ")\n",
    "\n",
    "#trim_messages() returns a Runnable (a LangChain ‚Äúmini-pipeline‚Äù object)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I‚Äôm not sure‚Äîcould you tell me a bit about your favorite flavors? For example, do you prefer classic vanilla, chocolate, or something more adventurous like salted caramel or pistachio? Any ingredients you love or dislike? That‚Äôll help me guess!'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What ice cream do i like\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You haven‚Äôt asked a math problem yet‚Äîjust chatting! If you have one in mind, feel free to share it.'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You asked: **‚ÄúWhat is 2‚ÄØ+‚ÄØ2?‚Äù**\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets wrap this in the MEssage History\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I don‚Äôt know‚Äîcould you let me know what you‚Äôd like to be called?'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I‚Äôm not sure which math problem you‚Äôre referring to. Could you let me know the details or the question you had in mind? That way I can help you more accurately.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what math problem did i ask?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Concept          | Tool / Class                 | Purpose                           |\n",
    "| ---------------- | ---------------------------- | --------------------------------- |\n",
    "| Model            | `ChatGroq`                   | Connects to Groq API              |\n",
    "| Stateful memory  | `RunnableWithMessageHistory` | Keeps chat context                |\n",
    "| Prompts          | `ChatPromptTemplate`         | Defines structure + system role   |\n",
    "| History trimming | `trim_messages()`            | Prevents context overflow         |\n",
    "| Multi-language   | `{language}` in prompt       | Makes responses language-specific |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
