{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae95606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f3c76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Addon'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a95342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking And Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47124a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x00000259041ECB20> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000259041ECA30> root_client=<openai.OpenAI object at 0x00000259041ECE50> root_async_client=<openai.AsyncOpenAI object at 0x00000259041ECD00> model_kwargs={} openai_api_key=SecretStr('**********') stream_usage=True\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "print(llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35ca146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Agentic AI refers to artificial intelligence systems that are designed to mimic human intelligence and behavior, often with a focus on achieving specific goals or tasks. These systems are usually programmed to act autonomously and make decisions based on the information they are given. Agentic AI is commonly used in a wide range of applications, including robotics, virtual assistants, and autonomous vehicles.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 12, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CVvJbvbw8Jjr5KcXQZ6SWz02XVAR4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--07613a88-69a0-42c3-8b6b-d6aed7d488b2-0' usage_metadata={'input_tokens': 12, 'output_tokens': 72, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is Agentic AI\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d6b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic AI refers to artificial intelligence systems that are designed to mimic human intelligence and behavior, often with a focus on achieving specific goals or tasks. These systems are usually programmed to act autonomously and make decisions based on the information they are given. Agentic AI is commonly used in a wide range of applications, including robotics, virtual assistants, and autonomous vehicles.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8357dd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Shaheer! Nice to meet you. How can I help you today?', additional_kwargs={'reasoning_content': 'We need to respond. The user says \"Hi My name is Shaheer\". Probably we should greet back and ask how can help. Follow guidelines: be friendly.'}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 77, 'total_tokens': 136, 'completion_time': 0.116402238, 'prompt_time': 0.002760748, 'queue_time': 0.003828078, 'total_time': 0.119162986}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_cbd208ac51', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--5c04aa4b-4445-4abc-a351-2d9d67bc8cdb-0', usage_metadata={'input_tokens': 77, 'output_tokens': 59, 'total_tokens': 136})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "model.invoke(\"Hi My name is Shaheer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f7253",
   "metadata": {},
   "source": [
    "### Prompts - Instructions to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0b56c",
   "metadata": {},
   "source": [
    "ChatPromptTemplate?\n",
    "\n",
    "* A specialized prompt template for chat-based LLMs.\n",
    "* Works with messages instead of a single string.\n",
    "* Each message has a role: system, user, assistant.\n",
    "* Allows structured chat interactions and multi-turn prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88e170b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer in one sentence based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer in one sentence based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6675071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# System message: sets the behavior of the assistant\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful assistant who answers questions based on the context provided.\"\n",
    ")\n",
    "\n",
    "# Human message: user question + context\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Combine into a chat prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34d2a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d414adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002590438D360>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002590438F760>, model_name='openai/gpt-oss-20b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429e1cb",
   "metadata": {},
   "source": [
    "## Chains - Connecting components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe57de",
   "metadata": {},
   "source": [
    "LangChain “Runnable / chaining” syntax using the pipe (|) operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbcf7df",
   "metadata": {},
   "source": [
    "What is chaining with |?\n",
    "\n",
    "* In the new Runnable API, any Runnable object can be connected to another using |.\n",
    "\n",
    "* It’s like function composition: the output of the left Runnable becomes the input of the right Runnable.\n",
    "\n",
    "* Makes workflows modular, readable, and concise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01978c57",
   "metadata": {},
   "source": [
    "---\n",
    "Runnable as “anything that can take some input and give output”.\n",
    "It’s like a function or a machine:\n",
    "\n",
    "* You feed it something (input)\n",
    "\n",
    "* It processes it internally\n",
    "\n",
    "* It returns something (output)\n",
    "\n",
    "In LangChain, Runnables are modular blocks in an AI workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41586bf7",
   "metadata": {},
   "source": [
    "---\n",
    "Chained Runnables (|) :\n",
    "* | is function composition for Runnables.\n",
    "\n",
    "* Makes LangChain workflows shorter and modular.\n",
    "\n",
    "* Can be used with prompts, LLMs, retrievers, tools, chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0196898b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A LLM is a Master of Laws degree, typically focused on a specific area of law.\n"
     ]
    }
   ],
   "source": [
    "#from langchain.chat_models import ChatOpenAI\n",
    "#llm = ChatOpenAI(model=\"openai/gpt-oss-120b\", temperature=0)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "chain = prompt | llm  # connect the template to LLM\n",
    "response = chain.invoke({\"input\": \"what is a llm\"})\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d40b1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001E1827B2500>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001E1857CB7C0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### chaining\n",
    "chain=prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a293571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI engineer, I'm familiar with LangSmith. It's an open-source platform designed to simplify the process of building and deploying large language models (LLMs). \n",
      "\n",
      "Here's a breakdown of what makes LangSmith noteworthy:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **User-Friendly Interface:** LangSmith boasts a web-based interface, making it accessible to a wider range of users, even those without extensive coding experience.\n",
      "* **Modular Design:** The platform is built with modularity in mind, allowing users to easily combine and customize different components to suit their specific needs.\n",
      "* **Fine-Tuning Capabilities:** LangSmith provides tools for fine-tuning pre-trained LLMs on custom datasets, enabling users to tailor models for specific tasks or domains.\n",
      "* **Experiment Tracking:** It includes features for tracking experiments and comparing different model configurations, facilitating the iterative development process.\n",
      "* **Community Driven:** Being open-source, LangSmith benefits from a vibrant community of developers and researchers who contribute to its growth and improvement.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Lower Barrier to Entry:** LangSmith democratizes access to LLM development by providing a more accessible and user-friendly platform.\n",
      "* **Increased Efficiency:** Its modular design and automation features streamline the development workflow, saving time and effort.\n",
      "* **Customization and Flexibility:** Users can fine-tune models and tailor them to their specific use cases, achieving better performance and accuracy.\n",
      "* **Collaboration and Innovation:** The open-source nature fosters collaboration and encourages the sharing of knowledge and best practices.\n",
      "\n",
      "**Use Cases:**\n",
      "\n",
      "LangSmith finds applications in various domains, including:\n",
      "\n",
      "* **Chatbots and Conversational AI:**\n",
      "\n",
      "Developing customized chatbots for customer service, education, or entertainment.\n",
      "* **Text Generation:**\n",
      "\n",
      "Creating compelling content, such as articles, stories, or marketing copy.\n",
      "* **Language Translation:**\n",
      "\n",
      "Fine-tuning models for accurate and efficient language translation.\n",
      "* **Code Generation:**\n",
      "\n",
      "Assisting developers in generating code snippets or completing code tasks.\n",
      "\n",
      "**Overall, LangSmith is a powerful and versatile platform that empowers individuals and organizations to leverage the potential of LLMs for a wide range of applications.**\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any more questions about LangSmith or LLMs in general!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\":\"Can you tell me something about Langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07aa35c",
   "metadata": {},
   "source": [
    "### Output Parser\n",
    "An OutputParser is a “post-processing tool” that takes raw text from an LLM and converts it into structured, usable data.\n",
    "\n",
    "* LLMs always return free-form text.\n",
    "\n",
    "* Often, you want data in a specific format: JSON, numbers, lists, or custom objects.\n",
    "\n",
    "* OutputParser reads the text and returns it in the desired format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd92c03",
   "metadata": {},
   "source": [
    "Why use OutputParser?\n",
    "\n",
    "* Ensures consistency in responses\n",
    "\n",
    "* Makes LLM outputs machine-readable\n",
    "\n",
    "* Allows further automation in pipelines\n",
    "\n",
    "Example: you want an LLM to return JSON with keys name and age, not just a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ccba71",
   "metadata": {},
   "source": [
    "---\n",
    "StrOutputParser is a very simple parser:\n",
    "\n",
    "It takes the raw LLM output and returns it as a string.\n",
    "\n",
    "No validation, no conversion — just ensures the chain returns a clean string rather than a raw LLM object.\n",
    "\n",
    "Useful when you don’t need structured data, just plain text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de91f2",
   "metadata": {},
   "source": [
    "---\n",
    "prompt → fills in input variables\n",
    "\n",
    "model → generates text from the prompt\n",
    "\n",
    "output_parser → formats the output (here, just converts to string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "295a06ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert in Metal prices. Provide me answer in one sentence based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert in Metal prices. Provide me answer in one sentence based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6694a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the current gold and silver prices: {'gold': 1950.12, 'silver': 24.56}\n"
     ]
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "#invoke() runs the entire chain in one step.\n",
    "\n",
    "chain=prompt|model|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"give the gold and siler price in json like {'gold':23}\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0221a0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66da8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fe079e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52a5b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gold': 1800, 'silver': 25}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"query\":\"give the gold and siler price in json format like {'gold':23,..}\"})\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee96082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed7d7e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': \"Langsmith is an open-source platform created by the team at Cohere designed to simplify the process of building and deploying custom AI applications.  \\n\\nHere are some key things to know about Langsmith:\\n\\n* **Focus on User Experience:** Langsmith prioritizes making AI development accessible to a wider audience, even those without extensive coding experience.\\n\\n* **Modular and Customizable:** It offers a modular architecture that allows users to easily integrate different AI models, tools, and components to create tailored solutions.\\n* **Streamlined Workflow:** Langsmith streamlines the development workflow by providing intuitive interfaces for tasks like data preparation, model training, and model deployment.\\n* **Emphasis on Collaboration:**  The platform encourages collaboration through features that enable teams to work together on AI projects.\\n* **Open Source:**  Being open-source, Langsmith fosters community contributions, transparency, and the ability to adapt the platform to specific needs.\\n\\n**Key Features:**\\n\\n* **Model Hub:** A repository of pre-trained AI models from various providers, including Cohere's own models.\\n* **Data Playground:** A space for cleaning, transforming, and visualizing data used for training AI models.\\n* **Experiment Tracker:**  Tools to track and compare the performance of different AI models and training configurations.\\n* **Deployment Tools:**  Options for deploying trained models as APIs or web applications.\\n\\n**Benefits:**\\n\\n* **Reduced Development Time:**  Langsmith's streamlined workflow and pre-built components accelerate the development process.\\n* **Improved Accessibility:**  The platform lowers the barrier to entry for individuals and teams new to AI development.\\n* **Increased Flexibility:**  The modular design allows for customization and the integration of various AI technologies.\\n\\n**Getting Started:**\\n\\nYou can find more information about Langsmith, including documentation, tutorials, and community resources, on the official Cohere website or their GitHub repository.\"}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4079b10",
   "metadata": {},
   "source": [
    "### Pydantic\n",
    "Pydantic is a Python library that lets you define structured data models and validate data automatically.\n",
    "\n",
    "* Think of it as a form template for your data.\n",
    "\n",
    "* You define what fields you expect, their types, and optionally constraints.\n",
    "\n",
    "* Pydantic ensures any input matches the model — otherwise it raises errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73c82df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice 30\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Person\nage\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='thirty', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/int_parsing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(p\u001b[38;5;241m.\u001b[39mname, p\u001b[38;5;241m.\u001b[39mage)  \u001b[38;5;66;03m# Alice 30\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Invalid input\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mPerson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAlice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthirty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ❌ raises validation error\u001b[39;00m\n",
      "File \u001b[1;32md:\\ApesAI\\1M_Addon_AIML\\venv\\lib\\site-packages\\pydantic\\main.py:250\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    249\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    252\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    256\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    257\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for Person\nage\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='thirty', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/int_parsing"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "# Valid input\n",
    "p = Person(name=\"Alice\", age=30)\n",
    "print(p.name, p.age)  # Alice 30\n",
    "\n",
    "# Invalid input\n",
    "p = Person(name=\"Alice\", age=\"thirty\")  # ❌ raises validation error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec255025",
   "metadata": {},
   "source": [
    "LLM outputs free text, \n",
    "You want it as a Python object with guaranteed types.\n",
    "\n",
    "You can use Pydantic with PydanticOutputParser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ff5e844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice 30\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Define the Pydantic model\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "llm_output = '{\"name\": \"Alice\", \"age\": 30}'\n",
    "person_obj = parser.parse(llm_output)\n",
    "\n",
    "print(person_obj.name, person_obj.age)  # Alice 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2999f98",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5ba921",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Problem Statement:\n",
    "\n",
    "Build an AI workflow that can:\n",
    "1. Greet a user by name in plain text.\n",
    "2. Generate structured data about a person (name and age).\n",
    "3. Answer user questions in a chat-style format.\n",
    "\n",
    "The workflow uses modular components (prompts, LLM, parsers) connected via a pipe (|) chain\n",
    "to ensure clean and structured outputs.\n",
    "\"\"\"\n",
    "\n",
    "# 1️ Imports\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# 2️ Define a Pydantic model for structured output\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "# 3️ Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "\n",
    "# 4️ Plain Text Example\n",
    "plain_prompt = PromptTemplate(\n",
    "    template=\"Say hello to {name} in one sentence.\",\n",
    "    input_variables=[\"name\"]\n",
    ")\n",
    "str_parser = StrOutputParser()\n",
    "str_chain = plain_prompt | llm | str_parser\n",
    "\n",
    "print(\"Plain Text Output:\")\n",
    "print(str_chain.invoke({\"name\": \"Alice\"}))  # Example: \"Hello Alice!\"\n",
    "\n",
    "# 5️ Structured Output Example\n",
    "person_parser = PydanticOutputParser(pydantic_object=Person)\n",
    "person_prompt = PromptTemplate(\n",
    "    template=\"Generate a person in JSON format with 'name' and 'age'.\",\n",
    "    input_variables=[]\n",
    ")\n",
    "person_chain = person_prompt | llm | person_parser\n",
    "\n",
    "print(\"\\nStructured Output (Pydantic):\")\n",
    "person = person_chain.invoke({})\n",
    "print(f\"Name: {person.name}, Age: {person.age}\")\n",
    "\n",
    "# 6️ Chat Prompt Example\n",
    "system_msg = SystemMessagePromptTemplate.from_template(\"You are a helpful assistant.\")\n",
    "human_msg = HumanMessagePromptTemplate.from_template(\"User question: {question}\")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
    "chat_chain = chat_prompt | llm | str_parser\n",
    "\n",
    "print(\"\\nChat Prompt Output:\")\n",
    "print(chat_chain.invoke({\"question\": \"What is Langsmith?\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27d50242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain Text Output:\n",
      "Hello, Alice!\n"
     ]
    }
   ],
   "source": [
    "# 1️ Imports\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# 3️ Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "\n",
    "# 4️ Plain Text Example\n",
    "plain_prompt = PromptTemplate(\n",
    "    template=\"Say hello to {name} in one sentence.\",\n",
    "    input_variables=[\"name\"]\n",
    ")\n",
    "\n",
    "str_parser = StrOutputParser()\n",
    "str_chain = plain_prompt | llm | str_parser\n",
    "\n",
    "print(\"Plain Text Output:\")\n",
    "print(str_chain.invoke({\"name\": \"Alice\"}))  # Example: \"Hello Alice!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9be1cdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Structured Output (Pydantic):\n",
      "Name: John Doe, Age: 30\n"
     ]
    }
   ],
   "source": [
    "# 1️ Imports\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# 2️ Define a Pydantic model for structured output\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "# 3️ Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "\n",
    "person_parser = PydanticOutputParser(pydantic_object=Person)\n",
    "person_prompt = PromptTemplate(\n",
    "    template=\"Generate a person in JSON format with 'name' and 'age'.\",\n",
    "    input_variables=[]\n",
    ")\n",
    "person_chain = person_prompt | llm | person_parser\n",
    "\n",
    "print(\"\\nStructured Output (Pydantic):\")\n",
    "person = person_chain.invoke({})\n",
    "print(f\"Name: {person.name}, Age: {person.age}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
