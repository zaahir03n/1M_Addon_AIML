{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ab913a",
   "metadata": {},
   "source": [
    "### Agentic RAG\n",
    "\n",
    "RAG + Decision-Making + Feedback Loop\n",
    "\n",
    "It’s not just retrieval — the model acts as an agent that can:\n",
    "* Decide when to use retrieval tools\n",
    "* Judge if retrieved info is relevant\n",
    "* Rewrite a question if results are poor\n",
    "* Generate a final answer from good info\n",
    "\n",
    "So instead of a fixed RAG pipeline, you have a looping decision graph that improves query handling intelligently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da2985b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGSMITH_API_KEY\"]=os.getenv(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64c96f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ApesAI\\1M_Addon_AIML\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "706e2335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='LangGraph overview - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nTrusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopyAsk AIpip install -U langgraph\\n\\nThen, create a simple hello world example:\\nCopyAsk AIfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\n\\nDurable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\n\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\n\\nLangSmith — Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\\nLangSmith — Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.\\nLangChain - Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\\n\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoWhat\\'s new in v1Next⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/application-structure', 'title': 'Application structure - Docs by LangChain', 'language': 'en'}, page_content='Application structure - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationProductionApplication structureLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageOverviewKey ConceptsFile structureConfiguration fileExamplesDependenciesGraphsEnvironment variablesProductionApplication structureCopy pageCopy page\\u200bOverview\\nA LangGraph application consists of one or more graphs, a configuration file (langgraph.json), a file that specifies dependencies, and an optional .env file that specifies environment variables.\\nThis guide shows a typical structure of an application and shows how the required information to deploy an application using the LangSmith is specified.\\n\\u200bKey Concepts\\nTo deploy using the LangSmith, the following information should be provided:\\n\\nA LangGraph configuration file (langgraph.json) that specifies the dependencies, graphs, and environment variables to use for the application.\\nThe graphs that implement the logic of the application.\\nA file that specifies dependencies required to run the application.\\nEnvironment variables that are required for the application to run.\\n\\n\\u200bFile structure\\nBelow are examples of directory structures for applications:\\n Python (requirements.txt) Python (pyproject.toml)CopyAsk AImy-app/\\n├── my_agent # all project code lies within here\\n│   ├── utils # utilities for your graph\\n│   │   ├── __init__.py\\n│   │   ├── tools.py # tools for your graph\\n│   │   ├── nodes.py # node functions for your graph\\n│   │   └── state.py # state definition of your graph\\n│   ├── __init__.py\\n│   └── agent.py # code for constructing your graph\\n├── .env # environment variables\\n├── requirements.txt # package dependencies\\n└── langgraph.json # configuration file for LangGraph\\n\\nThe directory structure of a LangGraph application can vary depending on the programming language and the package manager used.\\n\\n\\u200bConfiguration file\\nThe langgraph.json file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application.\\nSee the LangGraph configuration file reference for details on all supported keys in the JSON file.\\nThe LangGraph CLI defaults to using the configuration file langgraph.json in the current directory.\\n\\u200bExamples\\n\\nThe dependencies involve a custom local package and the langchain_openai package.\\nA single graph will be loaded from the file ./your_package/your_file.py with the variable variable.\\nThe environment variables are loaded from the .env file.\\n\\nCopyAsk AI{\\n  \"dependencies\": [\"langchain_openai\", \"./your_package\"],\\n  \"graphs\": {\\n    \"my_agent\": \"./your_package/your_file.py:agent\"\\n  },\\n  \"env\": \"./.env\"\\n}\\n\\n\\u200bDependencies\\nA LangGraph application may depend on other Python packages.\\nYou will generally need to specify the following information for dependencies to be set up correctly:\\n\\n\\nA file in the directory that specifies the dependencies (e.g. requirements.txt, pyproject.toml, or package.json).\\n\\n\\nA dependencies key in the LangGraph configuration file that specifies the dependencies required to run the LangGraph application.\\n\\n\\nAny additional binaries or system libraries can be specified using dockerfile_lines key in the LangGraph configuration file.\\n\\n\\n\\u200bGraphs\\nUse the graphs key in the LangGraph configuration file to specify which graphs will be available in the deployed LangGraph application.\\nYou can specify one or more graphs in the configuration file. Each graph is identified by a name (which should be unique) and a path for either: (1) the compiled graph or (2) a function that makes a graph is defined.\\n\\u200bEnvironment variables\\nIf you’re working with a deployed LangGraph application locally, you can configure environment variables in the env key of the LangGraph configuration file.\\nFor a production deployment, you will typically want to configure the environment variables in the deployment environment.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoSubgraphsPreviousStudioNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='Memory - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesMemoryLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageAdd short-term memoryUse in productionUse in subgraphsAdd long-term memoryUse in productionUse semantic searchManage short-term memoryTrim messagesDelete messagesSummarize messagesManage checkpointsView thread stateView the history of the threadDelete all checkpoints for a threadPrebuilt memory toolsCapabilitiesMemoryCopy pageCopy pageAI applications need memory to share context across multiple interactions. In LangGraph, you can add two types of memory:\\n\\nAdd short-term memory as a part of your agent’s state to enable multi-turn conversations.\\nAdd long-term memory to store user-specific or application-level data across sessions.\\n\\n\\u200bAdd short-term memory\\nShort-term memory (thread-level persistence) enables agents to track multi-turn conversations. To add short-term memory:\\nCopyAsk AIfrom langgraph.checkpoint.memory import InMemorySaver  \\nfrom langgraph.graph import StateGraph\\n\\ncheckpointer = InMemorySaver()  \\n\\nbuilder = StateGraph(...)\\ngraph = builder.compile(checkpointer=checkpointer)  \\n\\ngraph.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! i am Bob\"}]},\\n    {\"configurable\": {\"thread_id\": \"1\"}},  \\n)\\n\\n\\u200bUse in production\\nIn production, use a checkpointer backed by a database:\\nCopyAsk AIfrom langgraph.checkpoint.postgres import PostgresSaver\\n\\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:  \\n    builder = StateGraph(...)\\n    graph = builder.compile(checkpointer=checkpointer)  \\n\\nExample: using Postgres checkpointerCopyAsk AIpip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres\\nYou need to call checkpointer.setup() the first time you’re using Postgres checkpointer Sync AsyncCopyAsk AIfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import StateGraph, MessagesState, START\\nfrom langgraph.checkpoint.postgres import PostgresSaver  \\n\\nmodel = init_chat_model(model=\"claude-haiku-4-5-20251001\")\\n\\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:  \\n    # checkpointer.setup()\\n\\n    def call_model(state: MessagesState):\\n        response = model.invoke(state[\"messages\"])\\n        return {\"messages\": response}\\n\\n    builder = StateGraph(MessagesState)\\n    builder.add_node(call_model)\\n    builder.add_edge(START, \"call_model\")\\n\\n    graph = builder.compile(checkpointer=checkpointer)  \\n\\n    config = {\\n        \"configurable\": {\\n            \"thread_id\": \"1\"\\n        }\\n    }\\n\\n    for chunk in graph.stream(\\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I\\'m bob\"}]},\\n        config,  \\n        stream_mode=\"values\"\\n    ):\\n        chunk[\"messages\"][-1].pretty_print()\\n\\n    for chunk in graph.stream(\\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s my name?\"}]},\\n        config,  \\n        stream_mode=\"values\"\\n    ):\\n        chunk[\"messages\"][-1].pretty_print()\\n\\nExample: using [MongoDB](https://pypi.org/project/langgraph-checkpoint-mongodb/) checkpointerCopyAsk AIpip install -U pymongo langgraph langgraph-checkpoint-mongodb\\nSetup\\nTo use the MongoDB checkpointer, you will need a MongoDB cluster. Follow this guide to create a cluster if you don’t already have one. Sync AsyncCopyAsk AIfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import StateGraph, MessagesState, START\\nfrom langgraph.checkpoint.mongodb import MongoDBSaver  \\n\\nmodel = init_chat_model(model=\"claude-haiku-4-5-20251001\")\\n\\nDB_URI = \"localhost:27017\"\\nwith MongoDBSaver.from_conn_string(DB_URI) as checkpointer:  \\n\\n    def call_model(state: MessagesState):\\n        response = model.invoke(state[\"messages\"])\\n        return {\"messages\": response}\\n\\n    builder = StateGraph(MessagesState)\\n    builder.add_node(call_model)\\n    builder.add_edge(START, \"call_model\")\\n\\n    graph = builder.compile(checkpointer=checkpointer)  \\n\\n    config = {\\n        \"configurable\": {\\n            \"thread_id\": \"1\"\\n        }\\n    }\\n\\n    for chunk in graph.stream(\\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I\\'m bob\"}]},\\n        config,  \\n        stream_mode=\"values\"\\n    ):\\n        chunk[\"messages\"][-1].pretty_print()\\n\\n    for chunk in graph.stream(\\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s my name?\"}]},\\n        config,  \\n        stream_mode=\"values\"\\n    ):\\n        chunk[\"messages\"][-1].pretty_print()\\n\\nExample: using [Redis](https://pypi.org/project/langgraph-checkpoint-redis/) checkpointerCopyAsk AIpip install -U langgraph langgraph-checkpoint-redis\\nYou need to call checkpointer.setup() the first time you’re using Redis checkpointer Sync AsyncCopyAsk AIfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import StateGraph, MessagesState, START\\nfrom langgraph.checkpoint.redis import RedisSaver  \\n\\nmodel = init_chat_model(model=\"claude-haiku-4-5-20251001\")\\n\\nDB_URI = \"redis://localhost:6379\"\\nwith RedisSaver.from_conn_string(DB_URI) as checkpointer:  \\n    # checkpointer.setup()\\n\\n    def call_model(state: MessagesState):\\n        response = model.invoke(state[\"messages\"])\\n        return {\"messages\": response}\\n\\n    builder = StateGraph(MessagesState)\\n    builder.add_node(call_model)\\n    builder.add_edge(START, \"call_model\")\\n\\n    graph = builder.compile(checkpointer=checkpointer)  \\n\\n    config = {\\n        \"configurable\": {\\n            \"thread_id\": \"1\"\\n        }\\n    }\\n\\n    for chunk in graph.stream(\\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I\\'m bob\"}]},\\n        config,  \\n        stream_mode=\"values\"\\n    ):\\n        chunk[\"messages\"][-1].pretty_print()\\n\\n    for chunk in graph.stream(\\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s my name?\"}]},\\n        config,  \\n        stream_mode=\"values\"\\n    ):\\n        chunk[\"messages\"][-1].pretty_print()\\n\\n\\u200bUse in subgraphs\\nIf your graph contains subgraphs, you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.\\nCopyAsk AIfrom langgraph.graph import START, StateGraph\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom typing import TypedDict\\n\\nclass State(TypedDict):\\n    foo: str\\n\\n# Subgraph\\n\\ndef subgraph_node_1(state: State):\\n    return {\"foo\": state[\"foo\"] + \"bar\"}\\n\\nsubgraph_builder = StateGraph(State)\\nsubgraph_builder.add_node(subgraph_node_1)\\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\\nsubgraph = subgraph_builder.compile()  \\n\\n# Parent graph\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"node_1\", subgraph)  \\nbuilder.add_edge(START, \"node_1\")\\n\\ncheckpointer = InMemorySaver()\\ngraph = builder.compile(checkpointer=checkpointer)  \\n\\nIf you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories.\\nCopyAsk AIsubgraph_builder = StateGraph(...)\\nsubgraph = subgraph_builder.compile(checkpointer=True)  \\n\\n\\u200bAdd long-term memory\\nUse long-term memory to store user-specific or application-specific data across conversations.\\nCopyAsk AIfrom langgraph.store.memory import InMemoryStore  \\nfrom langgraph.graph import StateGraph\\n\\nstore = InMemoryStore()  \\n\\nbuilder = StateGraph(...)\\ngraph = builder.compile(store=store)  \\n\\n\\u200bUse in production\\nIn production, use a store backed by a database:\\nCopyAsk AIfrom langgraph.store.postgres import PostgresStore\\n\\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\\nwith PostgresStore.from_conn_string(DB_URI) as store:  \\n    builder = StateGraph(...)\\n    graph = builder.compile(store=store)  \\n\\nExample: using Postgres storeCopyAsk AIpip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres\\nYou need to call store.setup() the first time you’re using Postgres store Sync AsyncCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import StateGraph, MessagesState, START\\nfrom langgraph.checkpoint.postgres import PostgresSaver\\nfrom langgraph.store.postgres import PostgresStore  \\nfrom langgraph.store.base import BaseStore\\n\\nmodel = init_chat_model(model=\"claude-haiku-4-5-20251001\")\\n\\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\\n\\nwith (\\n    PostgresStore.from_conn_string(DB_URI) as store,  \\n    PostgresSaver.from_conn_string(DB_URI) as checkpointer,\\n):\\n    # store.setup()\\n    # checkpointer.setup()\\n\\n    def call_model(\\n        state: MessagesState,\\n        config: RunnableConfig,\\n        *,\\n        store: BaseStore,  \\n    ):\\n        user_id = config[\"configurable\"][\"user_id\"]\\n        namespace = (\"memories\", user_id)\\n        memories = store.search(namespace, query=str(state[\"messages\"][-1].content))  \\n        info = \"\\\\n\".join([d.value[\"data\"] for d in memories])\\n        system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\\n\\n        # Store new memories if the user asks the model to remember\\n        last_message = state[\"messages\"][-1]\\n        if \"remember\" in last_message.content.lower():\\n            memory = \"User name is Bob\"\\n            store.put(namespace, str(uuid.uuid4()), {\"data\": memory})  \\n\\n        response = model.invoke(\\n            [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\\n        )\\n        return {\"messages\": response}\\n\\n    builder = StateGraph(MessagesState)\\n    builder.add_node(call_model)\\n    builder.add_edge(START, \"call_model\")\\n\\n    graph = builder.compile(\\n        checkpointer=checkpointer,\\n        store=store,  \\n    )\\n\\n    config = {\\n        \"configurable\": {\\n            \"thread_id\": \"1\",  \\n            \"user_id\": \"1\",  \\n        }\\n    }\\n    for chunk in graph.stream(\\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\\n        config,  \\n        stream_mode=\"values\",\\n    ):\\n        chunk[\"messages\"][-1].pretty_print()\\n\\n    config = {\\n        \"configurable\": {\\n            \"thread_id\": \"2\",  \\n            \"user_id\": \"1\",\\n        }\\n    }\\n\\n    for chunk in graph.stream(\\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\\n        config,  \\n        stream_mode=\"values\",\\n    ):\\n        chunk[\"messages\"][-1].pretty_print()\\n\\nExample: using [Redis](https://pypi.org/project/langgraph-checkpoint-redis/) storeCopyAsk AIpip install -U langgraph langgraph-checkpoint-redis\\nYou need to call store.setup() the first time you’re using Redis store Sync AsyncCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import StateGraph, MessagesState, START\\nfrom langgraph.checkpoint.redis import RedisSaver\\nfrom langgraph.store.redis import RedisStore  \\nfrom langgraph.store.base import BaseStore\\n\\nmodel = init_chat_model(model=\"claude-haiku-4-5-20251001\")\\n\\nDB_URI = \"redis://localhost:6379\"\\n\\nwith (\\n    RedisStore.from_conn_string(DB_URI) as store,  \\n    RedisSaver.from_conn_string(DB_URI) as checkpointer,\\n):\\n    store.setup()\\n    checkpointer.setup()\\n\\n    def call_model(\\n        state: MessagesState,\\n        config: RunnableConfig,\\n        *,\\n        store: BaseStore,  \\n    ):\\n        user_id = config[\"configurable\"][\"user_id\"]\\n        namespace = (\"memories\", user_id)\\n        memories = store.search(namespace, query=str(state[\"messages\"][-1].content))  \\n        info = \"\\\\n\".join([d.value[\"data\"] for d in memories])\\n        system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\\n\\n        # Store new memories if the user asks the model to remember\\n        last_message = state[\"messages\"][-1]\\n        if \"remember\" in last_message.content.lower():\\n            memory = \"User name is Bob\"\\n            store.put(namespace, str(uuid.uuid4()), {\"data\": memory})  \\n\\n        response = model.invoke(\\n            [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\\n        )\\n        return {\"messages\": response}\\n\\n    builder = StateGraph(MessagesState)\\n    builder.add_node(call_model)\\n    builder.add_edge(START, \"call_model\")\\n\\n    graph = builder.compile(\\n        checkpointer=checkpointer,\\n        store=store,  \\n    )\\n\\n    config = {\\n        \"configurable\": {\\n            \"thread_id\": \"1\",  \\n            \"user_id\": \"1\",  \\n        }\\n    }\\n    for chunk in graph.stream(\\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\\n        config,  \\n        stream_mode=\"values\",\\n    ):\\n        chunk[\"messages\"][-1].pretty_print()\\n\\n    config = {\\n        \"configurable\": {\\n            \"thread_id\": \"2\",  \\n            \"user_id\": \"1\",\\n        }\\n    }\\n\\n    for chunk in graph.stream(\\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\\n        config,  \\n        stream_mode=\"values\",\\n    ):\\n        chunk[\"messages\"][-1].pretty_print()\\n\\n\\u200bUse semantic search\\nEnable semantic search in your graph’s memory store to let graph agents search for items in the store by semantic similarity.\\nCopyAsk AIfrom langchain.embeddings import init_embeddings\\nfrom langgraph.store.memory import InMemoryStore\\n\\n# Create store with semantic search enabled\\nembeddings = init_embeddings(\"openai:text-embedding-3-small\")\\nstore = InMemoryStore(\\n    index={\\n        \"embed\": embeddings,\\n        \"dims\": 1536,\\n    }\\n)\\n\\nstore.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\\nstore.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\\n\\nitems = store.search(\\n    (\"user_123\", \"memories\"), query=\"I\\'m hungry\", limit=1\\n)\\n\\nLong-term memory with semantic searchCopyAsk AI\\nfrom langchain.embeddings import init_embeddings\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.store.base import BaseStore\\nfrom langgraph.store.memory import InMemoryStore\\nfrom langgraph.graph import START, MessagesState, StateGraph\\n\\nmodel = init_chat_model(\"gpt-4o-mini\")\\n\\n# Create store with semantic search enabled\\nembeddings = init_embeddings(\"openai:text-embedding-3-small\")\\nstore = InMemoryStore(\\n    index={\\n        \"embed\": embeddings,\\n        \"dims\": 1536,\\n    }\\n)\\n\\nstore.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\\nstore.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\\n\\ndef chat(state, *, store: BaseStore):\\n    # Search based on user\\'s last message\\n    items = store.search(\\n        (\"user_123\", \"memories\"), query=state[\"messages\"][-1].content, limit=2\\n    )\\n    memories = \"\\\\n\".join(item.value[\"text\"] for item in items)\\n    memories = f\"## Memories of user\\\\n{memories}\" if memories else \"\"\\n    response = model.invoke(\\n        [\\n            {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\\\n{memories}\"},\\n            *state[\"messages\"],\\n        ]\\n    )\\n    return {\"messages\": [response]}\\n\\n\\nbuilder = StateGraph(MessagesState)\\nbuilder.add_node(chat)\\nbuilder.add_edge(START, \"chat\")\\ngraph = builder.compile(store=store)\\n\\nfor message, metadata in graph.stream(\\n    input={\"messages\": [{\"role\": \"user\", \"content\": \"I\\'m hungry\"}]},\\n    stream_mode=\"messages\",\\n):\\n    print(message.content, end=\"\")\\n\\n\\u200bManage short-term memory\\nWith short-term memory enabled, long conversations can exceed the LLM’s context window. Common solutions are:\\n\\nTrim messages: Remove first or last N messages (before calling LLM)\\nDelete messages from LangGraph state permanently\\nSummarize messages: Summarize earlier messages in the history and replace them with a summary\\nManage checkpoints to store and retrieve message history\\nCustom strategies (e.g., message filtering, etc.)\\n\\nThis allows the agent to keep track of the conversation without exceeding the LLM’s context window.\\n\\u200bTrim messages\\nMost LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you’re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the strategy (e.g., keep the last max_tokens) to use for handling the boundary.\\nTo trim message history, use the trim_messages function:\\nCopyAsk AIfrom langchain_core.messages.utils import (  \\n    trim_messages,  \\n    count_tokens_approximately  \\n)  \\n\\ndef call_model(state: MessagesState):\\n    messages = trim_messages(  \\n        state[\"messages\"],\\n        strategy=\"last\",\\n        token_counter=count_tokens_approximately,\\n        max_tokens=128,\\n        start_on=\"human\",\\n        end_on=(\"human\", \"tool\"),\\n    )\\n    response = model.invoke(messages)\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState)\\nbuilder.add_node(call_model)\\n...\\n\\nFull example: trim messagesCopyAsk AIfrom langchain_core.messages.utils import (\\n    trim_messages,  \\n    count_tokens_approximately  \\n)\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import StateGraph, START, MessagesState\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\nsummarization_model = model.bind(max_tokens=128)\\n\\ndef call_model(state: MessagesState):\\n    messages = trim_messages(  \\n        state[\"messages\"],\\n        strategy=\"last\",\\n        token_counter=count_tokens_approximately,\\n        max_tokens=128,\\n        start_on=\"human\",\\n        end_on=(\"human\", \"tool\"),\\n    )\\n    response = model.invoke(messages)\\n    return {\"messages\": [response]}\\n\\ncheckpointer = InMemorySaver()\\nbuilder = StateGraph(MessagesState)\\nbuilder.add_node(call_model)\\nbuilder.add_edge(START, \"call_model\")\\ngraph = builder.compile(checkpointer=checkpointer)\\n\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\ngraph.invoke({\"messages\": \"hi, my name is bob\"}, config)\\ngraph.invoke({\"messages\": \"write a short poem about cats\"}, config)\\ngraph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\\nfinal_response = graph.invoke({\"messages\": \"what\\'s my name?\"}, config)\\n\\nfinal_response[\"messages\"][-1].pretty_print()\\nCopyAsk AI================================== Ai Message ==================================\\n\\nYour name is Bob, as you mentioned when you first introduced yourself.\\n\\n\\u200bDelete messages\\nYou can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history.\\nTo delete messages from the graph state, you can use the RemoveMessage. For RemoveMessage to work, you need to use a state key with add_messages reducer, like MessagesState.\\nTo remove specific messages:\\nCopyAsk AIfrom langchain.messages import RemoveMessage  \\n\\ndef delete_messages(state):\\n    messages = state[\"messages\"]\\n    if len(messages) > 2:\\n        # remove the earliest two messages\\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  \\n\\nTo remove all messages:\\nCopyAsk AIfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\\n\\ndef delete_messages(state):\\n    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  \\n\\nWhen deleting messages, make sure that the resulting message history is valid. Check the limitations of the LLM provider you’re using. For example:\\nsome providers expect message history to start with a user message\\nmost providers require assistant messages with tool calls to be followed by corresponding tool result messages.\\n\\nFull example: delete messagesCopyAsk AIfrom langchain.messages import RemoveMessage  \\n\\ndef delete_messages(state):\\n    messages = state[\"messages\"]\\n    if len(messages) > 2:\\n        # remove the earliest two messages\\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  \\n\\ndef call_model(state: MessagesState):\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": response}\\n\\nbuilder = StateGraph(MessagesState)\\nbuilder.add_sequence([call_model, delete_messages])\\nbuilder.add_edge(START, \"call_model\")\\n\\ncheckpointer = InMemorySaver()\\napp = builder.compile(checkpointer=checkpointer)\\n\\nfor event in app.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I\\'m bob\"}]},\\n    config,\\n    stream_mode=\"values\"\\n):\\n    print([(message.type, message.content) for message in event[\"messages\"]])\\n\\nfor event in app.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s my name?\"}]},\\n    config,\\n    stream_mode=\"values\"\\n):\\n    print([(message.type, message.content) for message in event[\"messages\"]])\\nCopyAsk AI[(\\'human\\', \"hi! I\\'m bob\")]\\n[(\\'human\\', \"hi! I\\'m bob\"), (\\'ai\\', \\'Hi Bob! How are you doing today? Is there anything I can help you with?\\')]\\n[(\\'human\\', \"hi! I\\'m bob\"), (\\'ai\\', \\'Hi Bob! How are you doing today? Is there anything I can help you with?\\'), (\\'human\\', \"what\\'s my name?\")]\\n[(\\'human\\', \"hi! I\\'m bob\"), (\\'ai\\', \\'Hi Bob! How are you doing today? Is there anything I can help you with?\\'), (\\'human\\', \"what\\'s my name?\"), (\\'ai\\', \\'Your name is Bob.\\')]\\n[(\\'human\\', \"what\\'s my name?\"), (\\'ai\\', \\'Your name is Bob.\\')]\\n\\n\\u200bSummarize messages\\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\\n\\nPrompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can extend the MessagesState to include a summary key:\\nCopyAsk AIfrom langgraph.graph import MessagesState\\nclass State(MessagesState):\\n    summary: str\\n\\nThen, you can generate a summary of the chat history, using any existing summary as context for the next summary. This summarize_conversation node can be called after some number of messages have accumulated in the messages state key.\\nCopyAsk AIdef summarize_conversation(state: State):\\n\\n    # First, we get any existing summary\\n    summary = state.get(\"summary\", \"\")\\n\\n    # Create our summarization prompt\\n    if summary:\\n\\n        # A summary already exists\\n        summary_message = (\\n            f\"This is a summary of the conversation to date: {summary}\\\\n\\\\n\"\\n            \"Extend the summary by taking into account the new messages above:\"\\n        )\\n\\n    else:\\n        summary_message = \"Create a summary of the conversation above:\"\\n\\n    # Add prompt to our history\\n    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\\n    response = model.invoke(messages)\\n\\n    # Delete all but the 2 most recent messages\\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\\n    return {\"summary\": response.content, \"messages\": delete_messages}\\n\\nFull example: summarize messagesCopyAsk AIfrom typing import Any, TypedDict\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain.messages import AnyMessage\\nfrom langchain_core.messages.utils import count_tokens_approximately\\nfrom langgraph.graph import StateGraph, START, MessagesState\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langmem.short_term import SummarizationNode, RunningSummary  \\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\nsummarization_model = model.bind(max_tokens=128)\\n\\nclass State(MessagesState):\\n    context: dict[str, RunningSummary]  \\n\\nclass LLMInputState(TypedDict):  \\n    summarized_messages: list[AnyMessage]\\n    context: dict[str, RunningSummary]\\n\\nsummarization_node = SummarizationNode(  \\n    token_counter=count_tokens_approximately,\\n    model=summarization_model,\\n    max_tokens=256,\\n    max_tokens_before_summary=256,\\n    max_summary_tokens=128,\\n)\\n\\ndef call_model(state: LLMInputState):  \\n    response = model.invoke(state[\"summarized_messages\"])\\n    return {\"messages\": [response]}\\n\\ncheckpointer = InMemorySaver()\\nbuilder = StateGraph(State)\\nbuilder.add_node(call_model)\\nbuilder.add_node(\"summarize\", summarization_node)  \\nbuilder.add_edge(START, \"summarize\")\\nbuilder.add_edge(\"summarize\", \"call_model\")\\ngraph = builder.compile(checkpointer=checkpointer)\\n\\n# Invoke the graph\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\ngraph.invoke({\"messages\": \"hi, my name is bob\"}, config)\\ngraph.invoke({\"messages\": \"write a short poem about cats\"}, config)\\ngraph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\\nfinal_response = graph.invoke({\"messages\": \"what\\'s my name?\"}, config)\\n\\nfinal_response[\"messages\"][-1].pretty_print()\\nprint(\"\\\\nSummary:\", final_response[\"context\"][\"running_summary\"].summary)\\n\\nWe will keep track of our running summary in the context field\\n(expected by the SummarizationNode).\\nDefine private state that will be used only for filtering\\nthe inputs to call_model node.\\nWe’re passing a private input state here to isolate the messages returned by the summarization node\\nCopyAsk AI================================== Ai Message ==================================\\n\\nFrom our conversation, I can see that you introduced yourself as Bob. That\\'s the name you shared with me when we began talking.\\n\\nSummary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled \"The Mystery of Cats\" that captured cats\\' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote \"The Joy of Dogs,\" which highlighted dogs\\' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special.\\n\\n\\u200bManage checkpoints\\nYou can view and delete the information stored by the checkpointer.\\n\\n\\u200bView thread state\\n Graph/Functional API Checkpointer APICopyAsk AIconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\",  \\n        # optionally provide an ID for a specific checkpoint,\\n        # otherwise the latest checkpoint is shown\\n        # \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"  #\\n\\n    }\\n}\\ngraph.get_state(config)  \\nCopyAsk AIStateSnapshot(\\n    values={\\'messages\\': [HumanMessage(content=\"hi! I\\'m bob\"), AIMessage(content=\\'Hi Bob! How are you doing today?), HumanMessage(content=\"what\\'s my name?\"), AIMessage(content=\\'Your name is Bob.\\')]}, next=(),\\n    config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1f029ca3-1f5b-6704-8004-820c16b69a5a\\'}},\\n    metadata={\\n        \\'source\\': \\'loop\\',\\n        \\'writes\\': {\\'call_model\\': {\\'messages\\': AIMessage(content=\\'Your name is Bob.\\')}},\\n        \\'step\\': 4,\\n        \\'parents\\': {},\\n        \\'thread_id\\': \\'1\\'\\n    },\\n    created_at=\\'2025-05-05T16:01:24.680462+00:00\\',\\n    parent_config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1f029ca3-1790-6b0a-8003-baf965b6a38f\\'}},\\n    tasks=(),\\n    interrupts=()\\n)\\n\\n\\n\\u200bView the history of the thread\\n Graph/Functional API Checkpointer APICopyAsk AIconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\"\\n    }\\n}\\nlist(graph.get_state_history(config))  \\nCopyAsk AI[\\n    StateSnapshot(\\n        values={\\'messages\\': [HumanMessage(content=\"hi! I\\'m bob\"), AIMessage(content=\\'Hi Bob! How are you doing today? Is there anything I can help you with?\\'), HumanMessage(content=\"what\\'s my name?\"), AIMessage(content=\\'Your name is Bob.\\')]},\\n        next=(),\\n        config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1f029ca3-1f5b-6704-8004-820c16b69a5a\\'}},\\n        metadata={\\'source\\': \\'loop\\', \\'writes\\': {\\'call_model\\': {\\'messages\\': AIMessage(content=\\'Your name is Bob.\\')}}, \\'step\\': 4, \\'parents\\': {}, \\'thread_id\\': \\'1\\'},\\n        created_at=\\'2025-05-05T16:01:24.680462+00:00\\',\\n        parent_config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1f029ca3-1790-6b0a-8003-baf965b6a38f\\'}},\\n        tasks=(),\\n        interrupts=()\\n    ),\\n    StateSnapshot(\\n        values={\\'messages\\': [HumanMessage(content=\"hi! I\\'m bob\"), AIMessage(content=\\'Hi Bob! How are you doing today? Is there anything I can help you with?\\'), HumanMessage(content=\"what\\'s my name?\")]},\\n        next=(\\'call_model\\',),\\n        config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1f029ca3-1790-6b0a-8003-baf965b6a38f\\'}},\\n        metadata={\\'source\\': \\'loop\\', \\'writes\\': None, \\'step\\': 3, \\'parents\\': {}, \\'thread_id\\': \\'1\\'},\\n        created_at=\\'2025-05-05T16:01:23.863421+00:00\\',\\n        parent_config={...}\\n        tasks=(PregelTask(id=\\'8ab4155e-6b15-b885-9ce5-bed69a2c305c\\', name=\\'call_model\\', path=(\\'__pregel_pull\\', \\'call_model\\'), error=None, interrupts=(), state=None, result={\\'messages\\': AIMessage(content=\\'Your name is Bob.\\')}),),\\n        interrupts=()\\n    ),\\n    StateSnapshot(\\n        values={\\'messages\\': [HumanMessage(content=\"hi! I\\'m bob\"), AIMessage(content=\\'Hi Bob! How are you doing today? Is there anything I can help you with?\\')]},\\n        next=(\\'__start__\\',),\\n        config={...},\\n        metadata={\\'source\\': \\'input\\', \\'writes\\': {\\'__start__\\': {\\'messages\\': [{\\'role\\': \\'user\\', \\'content\\': \"what\\'s my name?\"}]}}, \\'step\\': 2, \\'parents\\': {}, \\'thread_id\\': \\'1\\'},\\n        created_at=\\'2025-05-05T16:01:23.863173+00:00\\',\\n        parent_config={...}\\n        tasks=(PregelTask(id=\\'24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd\\', name=\\'__start__\\', path=(\\'__pregel_pull\\', \\'__start__\\'), error=None, interrupts=(), state=None, result={\\'messages\\': [{\\'role\\': \\'user\\', \\'content\\': \"what\\'s my name?\"}]}),),\\n        interrupts=()\\n    ),\\n    StateSnapshot(\\n        values={\\'messages\\': [HumanMessage(content=\"hi! I\\'m bob\"), AIMessage(content=\\'Hi Bob! How are you doing today? Is there anything I can help you with?\\')]},\\n        next=(),\\n        config={...},\\n        metadata={\\'source\\': \\'loop\\', \\'writes\\': {\\'call_model\\': {\\'messages\\': AIMessage(content=\\'Hi Bob! How are you doing today? Is there anything I can help you with?\\')}}, \\'step\\': 1, \\'parents\\': {}, \\'thread_id\\': \\'1\\'},\\n        created_at=\\'2025-05-05T16:01:23.862295+00:00\\',\\n        parent_config={...}\\n        tasks=(),\\n        interrupts=()\\n    ),\\n    StateSnapshot(\\n        values={\\'messages\\': [HumanMessage(content=\"hi! I\\'m bob\")]},\\n        next=(\\'call_model\\',),\\n        config={...},\\n        metadata={\\'source\\': \\'loop\\', \\'writes\\': None, \\'step\\': 0, \\'parents\\': {}, \\'thread_id\\': \\'1\\'},\\n        created_at=\\'2025-05-05T16:01:22.278960+00:00\\',\\n        parent_config={...}\\n        tasks=(PregelTask(id=\\'8cbd75e0-3720-b056-04f7-71ac805140a0\\', name=\\'call_model\\', path=(\\'__pregel_pull\\', \\'call_model\\'), error=None, interrupts=(), state=None, result={\\'messages\\': AIMessage(content=\\'Hi Bob! How are you doing today? Is there anything I can help you with?\\')}),),\\n        interrupts=()\\n    ),\\n    StateSnapshot(\\n        values={\\'messages\\': []},\\n        next=(\\'__start__\\',),\\n        config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1f029ca3-0870-6ce2-bfff-1f3f14c3e565\\'}},\\n        metadata={\\'source\\': \\'input\\', \\'writes\\': {\\'__start__\\': {\\'messages\\': [{\\'role\\': \\'user\\', \\'content\\': \"hi! I\\'m bob\"}]}}, \\'step\\': -1, \\'parents\\': {}, \\'thread_id\\': \\'1\\'},\\n        created_at=\\'2025-05-05T16:01:22.277497+00:00\\',\\n        parent_config=None,\\n        tasks=(PregelTask(id=\\'d458367b-8265-812c-18e2-33001d199ce6\\', name=\\'__start__\\', path=(\\'__pregel_pull\\', \\'__start__\\'), error=None, interrupts=(), state=None, result={\\'messages\\': [{\\'role\\': \\'user\\', \\'content\\': \"hi! I\\'m bob\"}]}),),\\n        interrupts=()\\n    )\\n]\\n\\n\\u200bDelete all checkpoints for a thread\\nCopyAsk AIthread_id = \"1\"\\ncheckpointer.delete_thread(thread_id)\\n\\n\\u200bPrebuilt memory tools\\nLangMem is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the LangMem documentation for usage examples.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoUse time-travelPreviousSubgraphsNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=[\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/application-structure\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/add-memory\"\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in urls]\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a08aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "## Add alll these text to vectordb\n",
    "\n",
    "vectorstore=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "\n",
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "110bc8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-1.0.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain) (1.0.1)\n",
      "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
      "  Downloading langgraph-1.0.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain) (2.12.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (0.4.38)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.0)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading langgraph_prebuilt-1.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.11.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
      "Requirement already satisfied: certifi in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n",
      "Downloading langchain-1.0.3-py3-none-any.whl (91 kB)\n",
      "Downloading langgraph-1.0.2-py3-none-any.whl (156 kB)\n",
      "Downloading langgraph_prebuilt-1.0.2-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: langgraph-prebuilt, langgraph, langchain\n",
      "\n",
      "  Attempting uninstall: langgraph-prebuilt\n",
      "\n",
      "    Found existing installation: langgraph-prebuilt 1.0.1\n",
      "\n",
      "    Uninstalling langgraph-prebuilt-1.0.1:\n",
      "\n",
      "      Successfully uninstalled langgraph-prebuilt-1.0.1\n",
      "\n",
      "   ---------------------------------------- 0/3 [langgraph-prebuilt]\n",
      "  Attempting uninstall: langgraph\n",
      "   ---------------------------------------- 0/3 [langgraph-prebuilt]\n",
      "    Found existing installation: langgraph 1.0.1\n",
      "   ---------------------------------------- 0/3 [langgraph-prebuilt]\n",
      "    Uninstalling langgraph-1.0.1:\n",
      "   ---------------------------------------- 0/3 [langgraph-prebuilt]\n",
      "   ------------- -------------------------- 1/3 [langgraph]\n",
      "      Successfully uninstalled langgraph-1.0.1\n",
      "   ------------- -------------------------- 1/3 [langgraph]\n",
      "   ------------- -------------------------- 1/3 [langgraph]\n",
      "   ------------- -------------------------- 1/3 [langgraph]\n",
      "   ------------- -------------------------- 1/3 [langgraph]\n",
      "   ------------- -------------------------- 1/3 [langgraph]\n",
      "   ------------- -------------------------- 1/3 [langgraph]\n",
      "   ------------- -------------------------- 1/3 [langgraph]\n",
      "   ------------- -------------------------- 1/3 [langgraph]\n",
      "   ------------- -------------------------- 1/3 [langgraph]\n",
      "   -------------------------- ------------- 2/3 [langchain]\n",
      "   -------------------------- ------------- 2/3 [langchain]\n",
      "   -------------------------- ------------- 2/3 [langchain]\n",
      "   -------------------------- ------------- 2/3 [langchain]\n",
      "   ---------------------------------------- 3/3 [langchain]\n",
      "\n",
      "Successfully installed langchain-1.0.3 langgraph-1.0.2 langgraph-prebuilt-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbf5c710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_classic in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_classic) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_classic) (1.0.1)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_classic) (1.0.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.17 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_classic) (0.4.38)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_classic) (2.12.3)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_classic) (6.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_classic) (2.32.5)\n",
      "Requirement already satisfied: sqlalchemy<3.0.0,>=1.4.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain_classic) (2.0.44)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_classic) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_classic) (25.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_classic) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_classic) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain_classic) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.17->langchain_classic) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.17->langchain_classic) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.17->langchain_classic) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.17->langchain_classic) (0.25.0)\n",
      "Requirement already satisfied: anyio in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain_classic) (4.11.0)\n",
      "Requirement already satisfied: certifi in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain_classic) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain_classic) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain_classic) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain_classic) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain_classic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain_classic) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain_classic) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->langchain_classic) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->langchain_classic) (2.3.0)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from sqlalchemy<3.0.0,>=1.4.0->langchain_classic) (3.2.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain_classic) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\apesai\\1m_addon_aiml\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain_classic) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0a7a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retriever To Retriever Tools\n",
    "from langchain_classic.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool_1=create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_vector_langchain\", \n",
    "    \"Search and run information about Langgraph\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abcc6a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='retriever_vector_langchain', description='Search and run information about Langgraph', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000002044ADAD000>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002044B3F7310>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000002044A7E1000>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002044B3F7310>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ac6e08",
   "metadata": {},
   "source": [
    "### Langchain Blogs- Seperate Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c2ed2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/', 'title': 'LangChain overview - Docs by LangChain', 'language': 'en'}, page_content='LangChain overview - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\n\\u200b Install\\npipuvCopyAsk AIpip install -U langchain\\n\\n\\u200b Create an agent\\nCopyAsk AI# pip install -qU \"langchain[anthropic]\" to call the model\\n\\nfrom langchain.agents import create_agent\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nagent = create_agent(\\n    model=\"claude-sonnet-4-5-20250929\",\\n    tools=[get_weather],\\n    system_prompt=\"You are a helpful assistant\",\\n)\\n\\n# Run the agent\\nagent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\\n)\\n\\n\\u200b Core benefits\\nStandard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChain’s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChain’s agents are built on top of LangGraph. This allows us to take advantage of LangGraph’s durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoWhat\\'s new in v1Next⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'LangChain overview - Docs by LangChain', 'language': 'en'}, page_content='LangChain overview - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\n\\u200b Install\\npipuvCopyAsk AIpip install -U langchain\\n\\n\\u200b Create an agent\\nCopyAsk AI# pip install -qU \"langchain[anthropic]\" to call the model\\n\\nfrom langchain.agents import create_agent\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nagent = create_agent(\\n    model=\"claude-sonnet-4-5-20250929\",\\n    tools=[get_weather],\\n    system_prompt=\"You are a helpful assistant\",\\n)\\n\\n# Run the agent\\nagent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\\n)\\n\\n\\u200b Core benefits\\nStandard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChain’s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChain’s agents are built on top of LangGraph. This allows us to take advantage of LangGraph’s durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoWhat\\'s new in v1Next⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/qa_chat_history/', 'title': 'LangChain overview - Docs by LangChain', 'language': 'en'}, page_content='LangChain overview - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\n\\u200b Install\\npipuvCopyAsk AIpip install -U langchain\\n\\n\\u200b Create an agent\\nCopyAsk AI# pip install -qU \"langchain[anthropic]\" to call the model\\n\\nfrom langchain.agents import create_agent\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nagent = create_agent(\\n    model=\"claude-sonnet-4-5-20250929\",\\n    tools=[get_weather],\\n    system_prompt=\"You are a helpful assistant\",\\n)\\n\\n# Run the agent\\nagent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\\n)\\n\\n\\u200b Core benefits\\nStandard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChain’s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChain’s agents are built on top of LangGraph. This allows us to take advantage of LangGraph’s durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoWhat\\'s new in v1Next⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_urls=[\n",
    "    \"https://python.langchain.com/docs/tutorials/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/chatbot/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/qa_chat_history/\"\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in langchain_urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f834d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "## Add alll these text to vectordb\n",
    "\n",
    "vectorstorelangchain=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    ")\n",
    "\n",
    "\n",
    "retrieverlangchain=vectorstorelangchain.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf70c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool_2=create_retriever_tool(\n",
    "    retrieverlangchain,\n",
    "    \"retriever_vector_langchain\",\n",
    "    \"Search and run information about Langchain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edbc7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[retriever_tool_1,retriever_tool_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a665a36f",
   "metadata": {},
   "source": [
    "| Concept                 | **Retriever**                                                    | **Retriever Tool**                                                                 |\n",
    "| ----------------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------------------------- |\n",
    "| **What it is**          | A *function* that fetches relevant documents from a vector store | A *LangChain tool object* that wraps the retriever so an **LLM agent** can call it |\n",
    "| **Who uses it**         | You (the developer) or a static pipeline                         | The **LLM agent** during reasoning                                                 |\n",
    "| **Input / Output**      | Input: text query → Output: list of Documents                    | Input: text query → Output: same list of Documents, but via a “tool call”          |\n",
    "| **Role in Agentic RAG** | Performs retrieval (backend logic)                               | Exposes that retrieval as an *action* the agent can choose                         |\n",
    "| **How it’s created**    | `vectorstore.as_retriever()`                                     | `create_retriever_tool(retriever, name, description)`                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a65ca",
   "metadata": {},
   "source": [
    "### LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "127c309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208c2a9",
   "metadata": {},
   "source": [
    "It could be a list, tuple, or any ordered iterable — it’s more flexible and type-safe.\n",
    "\n",
    "Each element must be a subclass of BaseMessage (e.g., HumanMessage, AIMessage, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b581a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! 👋 How can I help you today?', additional_kwargs={'reasoning_content': 'We need to respond to a greeting. The user says \"Hi\". We can greet back, maybe ask how we can help.'}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 72, 'total_tokens': 119, 'completion_time': 0.04733882, 'prompt_time': 0.003401533, 'queue_time': 0.003179654, 'total_time': 0.050740353}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_80501ff3a1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--0dfb2c4b-c338-4466-b845-7436f30dcc8b-0', usage_metadata={'input_tokens': 72, 'output_tokens': 47, 'total_tokens': 119})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm=ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "llm.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5176f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8acb54d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGSMITH_API_KEY = os.getenv('LANGSMITH_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59080a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67749188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed80d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Edges\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    # Defines a small pydantic model grade to ensure output is \"yes\" or \"no\"\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "884aa9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated message\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    #prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    client = Client(api_key=LANGSMITH_API_KEY)\n",
    "    prompt = client.pull_prompt(\"rlm/rag-prompt\", include_model=True)\n",
    "    # LLM\n",
    "    llm = ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aa2a563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e6980",
   "metadata": {},
   "source": [
    "1. Agent node runs first\n",
    "* Decides if it needs to call a retrieval tool or not.\n",
    "\n",
    "2. Retrieve node runs (if agent decides “use tool”)\n",
    "* Calls the retriever to fetch documents.\n",
    "\n",
    "3. Grade_documents runs next\n",
    "* Checks if the retrieved documents are relevant.\n",
    "* Decision:\n",
    "    </br>\"yes\" → go to generate → produce answer.\n",
    "    </br>\"no\" → go to rewrite → produce a better question.\n",
    "4. Rewrite node runs (if docs weren’t relevant)\n",
    "* LLM reformulates the question.\n",
    "* Returns rewritten question to the agent, which can then retry retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "26bbd7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode([retriever_tool_1,retriever_tool_2])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\n",
    "    \"generate\", generate\n",
    ")  # Generating a response after we know the documents are relevant\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "661decb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is Langgraph?', additional_kwargs={}, response_metadata={}, id='b5c31a51-c1d7-4636-9196-77092c1affee'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'The user asks: \"What is Langgraph?\" We have a tool for searching information about Langgraph. We should use the function. There\\'s a function \"retriever_vector_langchain\" that searches. It seems both functions are same name but different namespace? Actually both are defined same name but maybe different? The first is \"namespace functions { // Search and run information about Langgraph type retriever_vector_langchain = ... }\" The second is same. It might be a mistake. We should call the function with query \"Langgraph\". The function returns any. We\\'ll call it.', 'tool_calls': [{'id': 'fc_b4c63fa3-3442-406d-b403-f6609ae89cb8', 'function': {'arguments': '{\"query\":\"Langgraph\"}', 'name': 'retriever_vector_langchain'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 146, 'prompt_tokens': 171, 'total_tokens': 317, 'completion_time': 0.147410556, 'prompt_time': 0.008265104, 'queue_time': 0.003167956, 'total_time': 0.15567566}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_80501ff3a1', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--618d6e72-e4f9-4c6d-896c-24d0855c67cc-0', tool_calls=[{'name': 'retriever_vector_langchain', 'args': {'query': 'Langgraph'}, 'id': 'fc_b4c63fa3-3442-406d-b403-f6609ae89cb8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 146, 'total_tokens': 317}),\n",
       "  ToolMessage(content=\"LangChain overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback,\\n\\nLangChain overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback,\\n\\nLangChain overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback,\\n\\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\n\\u200b Install\\npipuvCopyAsk AIpip install -U langchain\", name='retriever_vector_langchain', id='e742ae98-7c89-43b3-af9c-53fb07c644f2', tool_call_id='fc_b4c63fa3-3442-406d-b403-f6609ae89cb8'),\n",
       "  HumanMessage(content='LangGraph is a low‑level agent orchestration framework and runtime that powers advanced agent workflows. It lets developers combine deterministic and agentic steps, customize latency controls, and build complex, durable execution pipelines. LangChain agents are built on top of LangGraph, so you only need LangGraph when you need deeper customization and fine‑grained control.', additional_kwargs={}, response_metadata={}, id='6e42e460-9456-4cb8-b2d0-28aa97226374')]}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"What is Langgraph?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "448a376d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='langgrap hmemory', additional_kwargs={}, response_metadata={}, id='a8e60874-27ad-4454-967b-c59807d62876'),\n",
       "  AIMessage(content='## LangGraph Memory 101\\n\\nLangGraph treats **memory** as the “state” that flows through the graph.  \\nIt is what lets nodes see the conversation history, user context, and any\\narbitrary data you want to persist across turns.\\n\\nBelow is a quick‑start guide that covers:\\n\\n| Feature | What it does | Typical use‑case |\\n|---------|--------------|-----------------|\\n| **Node‑level memory** | Each node can read/write to a local key/value store. | Keep track of a user’s current “stage” or “intent”. |\\n| **Global (graph‑level) memory** | A shared store that all nodes can read/write. | Store a user profile, session ID, or a vector index. |\\n| **Memory Manager** | Handles persistence, serialization, and retrieval (e.g. RAG). | Store long‑term facts, retrieve relevant docs. |\\n| **Retrieval‑augmented memory** | Combines vector store lookup with the graph state. | Pull in domain knowledge or prior conversation snippets. |\\n| **Custom memory back‑ends** | Plug in Redis, Postgres, or a custom in‑memory dict. | Scale to many users or keep data off‑heap. |\\n\\n---\\n\\n## 1. The Core Memory API\\n\\n```python\\nfrom langgraph.memory import Memory\\n\\n# Create a simple in‑memory memory\\nmemory = Memory()\\n\\n# Store something\\nmemory.set(\"user_id\", \"alice\")\\n\\n# Retrieve it\\nprint(memory.get(\"user_id\"))   # → \"alice\"\\n\\n# Remove it\\nmemory.delete(\"user_id\")\\n```\\n\\n**Key methods**\\n\\n| Method | Purpose |\\n|--------|---------|\\n| `set(key, value)` | Persist a key/value pair. |\\n| `get(key, default=None)` | Fetch a value. |\\n| `delete(key)` | Remove a key. |\\n| `keys()` | List all keys. |\\n| `clear()` | Empty the memory. |\\n\\n---\\n\\n## 2. Node‑Level Memory (Local State)\\n\\nWhen you define a node in LangGraph, you get a `state` object that behaves like a `dict` but is backed by the memory manager.\\n\\n```python\\nfrom langgraph import State, Node, Graph\\n\\nclass MyNode(Node):\\n    def __call__(self, state: State):\\n        # Read\\n        stage = state.get(\"stage\", \"start\")\\n\\n        # Do something\\n        if stage == \"start\":\\n            state[\"stage\"] = \"ask_name\"\\n\\n        # Return updated state\\n        return state\\n\\ngraph = Graph(nodes=[MyNode()])\\n```\\n\\n*Each node gets its **own** copy of the state, but because the state is backed by the same `Memory` instance, writes are visible to subsequent nodes.*\\n\\n---\\n\\n## 3. Global Memory (Shared Across Nodes)\\n\\nYou can attach a memory object to the graph so that **all** nodes share it.\\n\\n```python\\nfrom langgraph import Graph\\nfrom langgraph.memory import Memory\\n\\nglobal_mem = Memory()\\n\\ngraph = Graph(memory=global_mem)\\n\\n# Now every node can access the same store\\n```\\n\\n> **Tip** – For multi‑user scenarios, store data under a user‑specific key (`f\"user:{user_id}:foo\"`).\\n\\n---\\n\\n## 4. Retrieval‑Augmented Memory (RAG)\\n\\nLangGraph ships with a **retriever memory** that can fetch vector‑indexed documents on demand.\\n\\n```python\\nfrom langgraph.memory import RetrievalMemory\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.vectorstores import FAISS\\n\\n# 1. Build a vector store\\nembeddings = OpenAIEmbeddings()\\ndocs = [\"Doc 1 text\", \"Doc 2 text\"]\\nfaiss = FAISS.from_texts(docs, embeddings)\\n\\n# 2. Create a retrieval memory\\nretrieval_mem = RetrievalMemory(\\n    retriever=faiss.as_retriever()\\n)\\n\\n# 3. Attach to graph\\ngraph = Graph(memory=retrieval_mem)\\n```\\n\\nNow, inside a node:\\n\\n```python\\ndef node(state: State):\\n    query = state.get(\"question\", \"\")\\n    # `retrieval_mem` automatically performs a vector search\\n    relevant_docs = state.get(\"retrieval\", [])\\n    # Use them in your logic\\n```\\n\\n> **Pro tip** – Pass `retrieval=True` when calling `state.get` to trigger a retrieval.\\n\\n---\\n\\n## 5. Custom Memory Back‑ends\\n\\nIf you need persistence across restarts or want to scale out, replace the default `Memory` with a custom store.\\n\\n```python\\nimport redis\\n\\nclass RedisMemory(Memory):\\n    def __init__(self, host=\"localhost\", port=6379, db=0):\\n        self.client = redis.Redis(host=host, port=port, db=db)\\n\\n    def set(self, key, value):\\n        self.client.set(key, pickle.dumps(value))\\n\\n    def get(self, key, default=None):\\n        raw = self.client.get(key)\\n        return pickle.loads(raw) if raw else default\\n\\n    def delete(self, key):\\n        self.client.delete(key)\\n\\n    def keys(self):\\n        return [k.decode() for k in self.client.keys()]\\n\\n    def clear(self):\\n        self.client.flushdb()\\n\\n# Use it\\nredis_mem = RedisMemory()\\ngraph = Graph(memory=redis_mem)\\n```\\n\\n---\\n\\n## 6. Common Patterns\\n\\n| Pattern | Code snippet |\\n|---------|--------------|\\n| **Session ID** | `state[\"session_id\"] = uuid4()` |\\n| **User profile** | `state[\"profile\"] = {\"name\": \"Alice\", \"age\": 30}` |\\n| **Conversation log** | `state.setdefault(\"log\", []).append(message)` |\\n| **Token budget tracking** | `state[\"tokens_used\"] += count_tokens(message)` |\\n\\n---\\n\\n## 7. Debugging & Inspection\\n\\n```python\\n# Print entire memory\\nprint(memory.get_all())\\n\\n# Inspect node‑specific state\\nfor node in graph.nodes:\\n    print(f\"{node.name}: {node.state}\")\\n```\\n\\n---\\n\\n## 8. Frequently Asked Questions\\n\\n| Question | Answer |\\n|----------|--------|\\n| *Can I store large objects (e.g., PDFs) in memory?* | Store a **pointer** or **hash**; keep the heavy data in a vector store or external DB. |\\n| *How do I clear memory after a conversation ends?* | Call `memory.clear()` or delete specific keys. |\\n| *Is memory thread‑safe?* | The default `Memory` is not thread‑safe; use a lock or a thread‑safe back‑end (Redis, Postgres). |\\n| *Can I persist memory across graph restarts?* | Yes, by using a persistent backend (Redis, Postgres, etc.). |\\n\\n---\\n\\n## 9. Quick‑start: A Complete Example\\n\\n```python\\nfrom langgraph import State, Node, Graph\\nfrom langgraph.memory import RetrievalMemory\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.vectorstores import FAISS\\nimport uuid\\n\\n# 1. Build vector store\\nembeddings = OpenAIEmbeddings()\\ndocs = [\"LangGraph is a framework for building LLM-powered workflows.\",\\n        \"Memory lets you keep state across steps.\"]\\nfaiss = FAISS.from_texts(docs, embeddings)\\n\\n# 2. Retrieval memory\\nretrieval_mem = RetrievalMemory(retriever=faiss.as_retriever())\\n\\n# 3. Graph definition\\nclass AskName(Node):\\n    def __call__(self, state: State):\\n        state[\"stage\"] = \"awaiting_name\"\\n        return state\\n\\nclass CaptureName(Node):\\n    def __call__(self, state: State):\\n        # Pretend we got the name from the user\\n        state[\"name\"] = \"Alice\"\\n        return state\\n\\nclass Summarize(Node):\\n    def __call__(self, state: State):\\n        # Retrieve relevant docs\\n        relevant = state.get(\"retrieval\", [])\\n        summary = \" \".join([d.page_content for d in relevant])\\n        state[\"summary\"] = summary\\n        return state\\n\\ngraph = Graph(nodes=[AskName(), CaptureName(), Summarize()],\\n              memory=retrieval_mem)\\n\\n# 4. Run the graph\\nstate = State()\\nstate = graph(state)\\nprint(state)\\n```\\n\\n---\\n\\n## 10. Resources\\n\\n| Resource | Link |\\n|----------|------|\\n| LangGraph Docs | https://langgraph.org/docs |\\n| Memory API | https://langgraph.org/docs/memory |\\n| LangChain Retrieval | https://langchain.org/docs/modules/data_connection/vectorstores/ |\\n\\n---\\n\\n### Got a specific use‑case?\\n\\nLet me know what you’re building (e.g., a multi‑turn chatbot, a recommendation engine, a data‑driven workflow) and I can give you a tailored memory setup!', additional_kwargs={'reasoning_content': 'The user wrote \"langgrap hmemory\" likely typo for \"langgraph memory\". They likely want information about memory in LangGraph. We should respond with info: memory components, state, context, node memory, global memory, memory managers, etc. Also mention retrieval, vector store, usage. Provide code examples. Also mention integration with LangChain. Also mention the memory API. Provide best practices. Also ask if they want code. We\\'ll provide explanation.'}, response_metadata={'token_usage': {'completion_tokens': 1925, 'prompt_tokens': 171, 'total_tokens': 2096, 'completion_time': 1.98719851, 'prompt_time': 0.01143658, 'queue_time': 0.002982636, 'total_time': 1.99863509}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_80501ff3a1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--3afa896f-e73b-49ed-ae5f-51ee19541ddb-0', usage_metadata={'input_tokens': 171, 'output_tokens': 1925, 'total_tokens': 2096})]}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"langgrap hmemory\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c03cb88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is Machine learning?', additional_kwargs={}, response_metadata={}, id='179f0c66-4401-4380-95ea-9f2978ea555e'),\n",
       "  AIMessage(content='**Machine Learning (ML)** is a subfield of artificial intelligence that focuses on building systems that can learn patterns from data and improve their performance over time without being explicitly programmed for every possible situation.\\n\\n### Core ideas\\n\\n| Concept | What it means |\\n|---------|---------------|\\n| **Learning** | The system adapts its internal model based on past observations (data). |\\n| **Model** | A mathematical representation (e.g., a neural network, decision tree) that maps inputs to outputs. |\\n| **Training** | The process of feeding data to the model so it adjusts its parameters to reduce prediction error. |\\n| **Generalization** | The model’s ability to make accurate predictions on new, unseen data. |\\n| **Evaluation** | Measuring performance using metrics (accuracy, precision, recall, RMSE, etc.) on a test set. |\\n\\n### Types of Machine Learning\\n\\n| Type | Typical use‑case | Example |\\n|------|------------------|---------|\\n| **Supervised** | Predict a label or value given labeled examples. | Spam detection, house price prediction. |\\n| **Unsupervised** | Discover hidden structure in unlabeled data. | Customer segmentation, anomaly detection. |\\n| **Semi‑supervised** | Mix of labeled and unlabeled data to improve learning. | Image classification with few labeled images. |\\n| **Reinforcement** | Learn to take actions to maximize cumulative reward. | Game playing (AlphaGo), robotics control. |\\n| **Self‑supervised** | Create labels from the data itself (common in NLP and vision). | Masked language modeling (BERT), image inpainting. |\\n\\n### Key Steps in a Typical ML Project\\n\\n1. **Problem definition** – Translate a real‑world problem into a learnable task.\\n2. **Data collection & preprocessing** – Gather data, clean, transform, and split into train/validation/test sets.\\n3. **Feature engineering / representation** – Decide what inputs the model should see (raw pixels, text embeddings, engineered features).\\n4. **Model selection** – Choose an algorithm or architecture suitable for the task (e.g., linear regression, CNN, Transformer).\\n5. **Training** – Optimize the model’s parameters using a loss function and an optimization algorithm (e.g., SGD, Adam).\\n6. **Evaluation & validation** – Assess performance on held‑out data, tune hyper‑parameters, avoid overfitting.\\n7. **Deployment** – Integrate the model into an application or service, monitor for drift, and retrain as needed.\\n\\n### Why It Matters\\n\\n- **Automation**: ML can handle tasks that are too complex or tedious for humans to code explicitly.\\n- **Insight**: It can reveal hidden patterns and relationships in data that would be hard to detect manually.\\n- **Personalization**: From recommendation engines to adaptive learning systems, ML tailors experiences to individuals.\\n\\n### Quick Example (Python + Scikit‑Learn)\\n\\n```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score\\n\\n# 1. Load data\\nX, y = load_breast_cancer(return_X_y=True)\\n\\n# 2. Split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# 3. Scale\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test  = scaler.transform(X_test)\\n\\n# 4. Train\\nclf = LogisticRegression(max_iter=1000)\\nclf.fit(X_train, y_train)\\n\\n# 5. Evaluate\\npred = clf.predict(X_test)\\nprint(\"Accuracy:\", accuracy_score(y_test, pred))\\n```\\n\\nThis simple pipeline demonstrates the core workflow: data → preprocessing → model → training → evaluation.\\n\\n---\\n\\n**TL;DR**: Machine learning is the science of teaching computers to learn from data, enabling them to perform tasks—like classification, regression, clustering, or decision making—by identifying patterns and improving over time.', additional_kwargs={'reasoning_content': 'The user asks: \"What is Machine learning?\" They expect a concise explanation. We can provide definition, maybe mention types, examples, difference from AI, etc. Provide clarity.'}, response_metadata={'token_usage': {'completion_tokens': 892, 'prompt_tokens': 171, 'total_tokens': 1063, 'completion_time': 0.887698765, 'prompt_time': 0.009534357, 'queue_time': 0.002960235, 'total_time': 0.897233122}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--4d2702db-7aa6-44b1-b95e-3df4201c469d-0', usage_metadata={'input_tokens': 171, 'output_tokens': 892, 'total_tokens': 1063})]}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"What is Machine learning?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2427a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
